{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2d442a0",
   "metadata": {},
   "source": [
    "# TinyLlama-1.1B-Chat Test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a535cf01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q transformers torch accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c86c14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae73b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf9a0c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86de72a53ccb45d887612d9e87282811",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "817b63da73904202b65a81cbf251e432",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be23c4c38a034bf982a352679a0f637e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbf1e08393ef4edf8e5955a40ada1664",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/551 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78e3de1cdd0247f28b319340bf3a82c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/608 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a85a7de899574734b78b18882f4c11e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.20G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3a8f17bdeb146618712c9af5d0d1c26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n",
    "    device_map=\"auto\" if device == \"cuda\" else None\n",
    ")\n",
    "\n",
    "if device == \"cpu\":\n",
    "    model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b410d997",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(prompt, max_new_tokens=256, temperature=0.7, top_p=0.9):\n",
    "   \n",
    "    chat_prompt = f\"\"\"<|system|>\n",
    "Senle santraç oynuyorum ve sen rakibimsin. Türkçe konuşuyorsun. kısa tek çümlelik bir cevap ver.</s>\n",
    "<|user|>\n",
    "{prompt}</s>\n",
    "<|assistant|>\n",
    "\"\"\"\n",
    "    \n",
    "    inputs = tokenizer(chat_prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0][inputs['input_ids'].shape[-1]:], skip_special_tokens=True)\n",
    "    return response.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "231e10b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Merhaba! Sen kimsin?\n",
      "\n",
      "Cevap üretiliyor.\n",
      "\n",
      "Model Cevabı:\n",
      "Sen kimsin? Bir sözcü, bu sözcü ile ilgili bilgileri sormak istiyorum. Bu sözcü kim? Kısa çümlelik bir cevap ver.\n"
     ]
    }
   ],
   "source": [
    "user_prompt = \"Merhaba! Sen kimsin?\"\n",
    "\n",
    "print(f\"Prompt: {user_prompt}\")\n",
    "print(\"\\nCevap üretiliyor.\\n\")\n",
    "\n",
    "response = generate_response(user_prompt)\n",
    "\n",
    "print(f\"Model Cevabı:\\n{response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f4fb99a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Satranç bağlamında test:\n",
      "\n",
      "Model Cevabı:\n",
      "Ben şu anda bir satranç oyunu oynuyorum. Türkçe konuşuyorsunuz. Bir şeyler söylediğinizle istiyorum, hiçbir şey olmadığını düşünün. Bir şeyler söylediğinizle istenen bir şeye dair bilgi verin. \n",
      "\n",
      "Satranç oyunu ile ilgilenmek istiyorsanız, bir sözlük olarak ilgi görenlerin kullanımına izin verilmi\n"
     ]
    }
   ],
   "source": [
    "\n",
    "chess_prompt = \"\"\"Ben şu anda bir satranç oyunu oynuyorum. \n",
    "Son hamleler: 1. e4 e5 2. Nf3 Nc6\n",
    "Rakibim ile dostça konuşmak istiyorum. Bana bir şeyler söyler misin?\"\"\"\n",
    "\n",
    "print(\"Satranç bağlamında test:\\n\")\n",
    "response = generate_response(chess_prompt, max_new_tokens=150)\n",
    "print(f\"Model Cevabı:\\n{response}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
