{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2d442a0",
   "metadata": {},
   "source": [
    "# TinyLlama-1.1B-Chat Test\n",
    "Bu notebook Hugging Face'ten TinyLlama-1.1B-Chat-v1.0 modelini yÃ¼kler ve test eder.\n",
    "\n",
    "**âœ… Avantajlar:**\n",
    "- KÃ¼Ã§Ã¼k model (1.1B parametre)\n",
    "- CPU'da Ã§alÄ±ÅŸÄ±r (GPU ÅŸart deÄŸil)\n",
    "- HÄ±zlÄ± yÃ¼kleme ve cevap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a535cf01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gerekli kÃ¼tÃ¼phaneleri yÃ¼kle (TinyLlama iÃ§in quantization gerekmez)\n",
    "%pip install -q transformers torch accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c86c14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import'lar\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae73b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model ayarlarÄ±\n",
    "model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "\n",
    "# GPU kontrolÃ¼ (opsiyonel - CPU'da da Ã§alÄ±ÅŸÄ±r)\n",
    "print(f\"CUDA kullanÄ±labilir mi: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    print(\"CPU kullanÄ±lacak (TinyLlama iÃ§in yeterli)\")\n",
    "    device = \"cpu\"\n",
    "\n",
    "print(f\"\\nâœ“ Model yÃ¼klenecek: {model_name}\")\n",
    "print(f\"âœ“ Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf9a0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model yÃ¼kleme\n",
    "print(\"ğŸ“¦ Tokenizer yÃ¼kleniyor...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "print(\"ğŸ“¦ Model yÃ¼kleniyor...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n",
    "    device_map=\"auto\" if device == \"cuda\" else None\n",
    ")\n",
    "\n",
    "if device == \"cpu\":\n",
    "    model = model.to(device)\n",
    "\n",
    "print(\"\\nâœ… Model baÅŸarÄ±yla yÃ¼klendi!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b410d997",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt'a cevap Ã¼retme fonksiyonu\n",
    "def generate_response(prompt, max_new_tokens=256, temperature=0.7, top_p=0.9):\n",
    "    \"\"\"\n",
    "    Verilen prompt'a model cevabÄ± Ã¼retir.\n",
    "    \n",
    "    Args:\n",
    "        prompt: KullanÄ±cÄ± mesajÄ±\n",
    "        max_new_tokens: Maksimum Ã¼retilecek token sayÄ±sÄ±\n",
    "        temperature: YaratÄ±cÄ±lÄ±k (0.1-1.0, yÃ¼ksek = daha yaratÄ±cÄ±)\n",
    "        top_p: Nucleus sampling parametresi\n",
    "    \"\"\"\n",
    "    # TinyLlama chat format\n",
    "    chat_prompt = f\"\"\"<|system|>\n",
    "Sen yardÄ±mcÄ± bir asistansÄ±n. TÃ¼rkÃ§e konuÅŸuyorsun.</s>\n",
    "<|user|>\n",
    "{prompt}</s>\n",
    "<|assistant|>\n",
    "\"\"\"\n",
    "    \n",
    "    # Tokenize et\n",
    "    inputs = tokenizer(chat_prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    # Generate\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    # Decode - sadece yeni Ã¼retilen kÄ±smÄ± al\n",
    "    response = tokenizer.decode(outputs[0][inputs['input_ids'].shape[-1]:], skip_special_tokens=True)\n",
    "    return response.strip()\n",
    "\n",
    "print(\"âœ“ generate_response() fonksiyonu hazÄ±r!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c5f62f",
   "metadata": {},
   "source": [
    "## Test AlanÄ±\n",
    "AÅŸaÄŸÄ±daki hÃ¼creyi Ã§alÄ±ÅŸtÄ±rarak modelinizi test edin. Ä°stediÄŸiniz prompt'u yazabilirsiniz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231e10b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elle prompt yazÄ±n ve Ã§alÄ±ÅŸtÄ±rÄ±n\n",
    "user_prompt = \"Merhaba! Sen kimsin?\"\n",
    "\n",
    "print(f\"Prompt: {user_prompt}\")\n",
    "print(\"\\nCevap Ã¼retiliyor...\\n\")\n",
    "\n",
    "response = generate_response(user_prompt)\n",
    "\n",
    "print(f\"Model CevabÄ±:\\n{response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4fb99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SatranÃ§ baÄŸlamÄ±nda test\n",
    "chess_prompt = \"\"\"Ben ÅŸu anda bir satranÃ§ oyunu oynuyorum. \n",
    "Son hamleler: 1. e4 e5 2. Nf3 Nc6\n",
    "Rakibim ile dostÃ§a konuÅŸmak istiyorum. Bana bir ÅŸeyler sÃ¶yler misin?\"\"\"\n",
    "\n",
    "print(\"SatranÃ§ baÄŸlamÄ±nda test:\\n\")\n",
    "response = generate_response(chess_prompt, max_new_tokens=150)\n",
    "print(f\"Model CevabÄ±:\\n{response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71aad15",
   "metadata": {},
   "source": [
    "## Notlar\n",
    "\n",
    "### TinyLlama AvantajlarÄ±:\n",
    "- **KÃ¼Ã§Ã¼k**: Sadece 1.1B parametre (~2.2 GB)\n",
    "- **HÄ±zlÄ±**: CPU'da bile Ã§alÄ±ÅŸÄ±r\n",
    "- **Kolay**: Token veya Ã¶zel eriÅŸim gerektirmez\n",
    "- **Ãœcretsiz**: Hugging Face'ten direkt indirilebilir\n",
    "\n",
    "### Sistem gereksinimleri:\n",
    "- **RAM**: 4-8 GB yeterli\n",
    "- **GPU**: Opsiyonel (varsa daha hÄ±zlÄ±)\n",
    "- **Disk**: ~3 GB model dosyasÄ±\n",
    "\n",
    "### Parametreler:\n",
    "- `temperature`: 0.1-1.0 (dÃ¼ÅŸÃ¼k = tutarlÄ±, yÃ¼ksek = yaratÄ±cÄ±)\n",
    "- `max_new_tokens`: Maksimum cevap uzunluÄŸu (token sayÄ±sÄ±)\n",
    "- `top_p`: 0.1-1.0 (nucleus sampling)\n",
    "\n",
    "### Model performansÄ±:\n",
    "- KÃ¼Ã§Ã¼k model olduÄŸu iÃ§in bÃ¼yÃ¼k modeller kadar gÃ¼Ã§lÃ¼ deÄŸil\n",
    "- Basit sohbet ve talimatlar iÃ§in yeterli\n",
    "- SatranÃ§ sohbeti iÃ§in uygun"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
